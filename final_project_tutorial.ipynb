{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44afbef3",
   "metadata": {},
   "source": [
    "# Final Project Starting Guide\n",
    "\n",
    "Hello everyone, welcome to the final project! This notebook is provided to you to reiterate the rules and guidelines, and give you some starting points.\n",
    "\n",
    "### What we provide\n",
    "\n",
    "In this project, we will provide you with \n",
    "- This starting guide\n",
    "- A working API that you can access under ASU network (i.e., on campus or with VPN)\n",
    "- A starting development data that you can use to develop your agent. It contains 1,000 instances with {domain, input, expected_output}\n",
    "\n",
    "### Your goal\n",
    "\n",
    "In this project, you will implement an inference-time agent to solve reasoning requests, as those provided in the development data. The grading of this project will be effort-based and you will get full credit if you produce the minimum deliverables below, with subject to the rules and requirements below.\n",
    "\n",
    "#### Minimum Deliverables\n",
    "\n",
    "1. A working agent loop (in the form of a Github project) that the TA can run, and implements *at least three* inference-time algorithms or techniques.\n",
    "2. Outputs from your agent on the released test data (see important dates). \n",
    "3. A short one-page report on how your agent works, and pointer to important techniques (referece to code blocks).\n",
    "\n",
    "#### Rules and Requirements\n",
    "1. You must only use our provided API call to access LLMs; meaning that you cannot use any other LLMs in any other way within your agent loop. Some exceptions may be made if you call certain external tools (e.g., Google search) that use some LLMs internally. Please discuss any exceptions with us to avoid penalties up to 50% of the project grade.\n",
    "2. You must not hardcode a full delegation to an external tool (e.g., google_search(input_problem)). Such delegations must be automatically selected/decided by your agent. Hardcode delegations will lead to a zero.\n",
    "3. You cannot use Cursor or any AI coding aids to implement the final project. You can, however, ask LLMs (or other online resources) for conceptual clarification or code examples. Your final project should not contain any blocks of code (i.e., > 3 lines) that are written by AI. Violations will lead to a zero.\n",
    "4. Your agent should be able to run efficiently, with <20 LLM calls per question. Exceptions may be made when you have a complicated agent but please discuss with us. Up to 10% of the project grade may be deducted if we observe very inefficient LLM usages that do not clearly benefit the performance.\n",
    "5. Your agent must run without any requests to any paid services (paid is defined by if the TA has to pay to run it, regardless of whether you actuallly pay for it or not.) Violations will lead to a zero.\n",
    "6. You must submit a Github project link as your code submission. All changes must be tracked and any commits should be within 100 lines of +/- with good messages. Points will be deducted to up to 25% of the project grade if we observe \"magic commits\" or too few commits. \n",
    "\n",
    "\n",
    "### Suggestions\n",
    "1. Start early, please.\n",
    "2. You should consider how you can evaluate whether your output is good enough compared to the provided expected_outputs, and we will not release how we will actually evaluate your outputs; meaning that you have to try to predict how we will evaluate things.\n",
    "3. Start with a basic implementation, and iterate based on mistakes/feedbacks.\n",
    "4. Find more development data, or create your own cases to stree-test your agent. \n",
    "5. You are free to modify any provided code in this starting guide, or not using any of these code at all.\n",
    "\n",
    "### Important dates\n",
    "- **Release of final test data**: 11/25/2025\n",
    "- **Deadline for submitting all deliverables**: 12/05/2025\n",
    "\n",
    "### Extra Credit. \n",
    "The top 20 projects (ranked by performance metrics on the test data and at the TA's discretion of implementation quality) will be given extra credits. The actual credits will be between 1% to 7.5% depending on the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7858ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "# API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "# API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "# MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")   \n",
    "\n",
    "API_KEY = \"cse476\"\n",
    "API_BASE = \"http://10.4.58.53:41701/v1\"\n",
    "MODEL = \"bens_model\"           \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer—no explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.0,\n",
    "                                timeout: int = 60) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": 128,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6362f9",
   "metadata": {},
   "source": [
    "## 1) Smoke test: direct inference\n",
    "\n",
    "We’ll do a single request with a strict instruction to answer briefly.  \n",
    "*If you see an auth error, set `OPENAI_API_KEY` and (if needed) `API_BASE`/`MODEL_NAME`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c02ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: True HTTP: 200\n",
      "MODEL SAYS: 45\n"
     ]
    }
   ],
   "source": [
    "# %% Direct call example\n",
    "demo_prompt = \"What is 17 + 28? Answer with just the number.\"\n",
    "result = call_model_chat_completions(demo_prompt)\n",
    "print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "# Optional: Inspect rate-limit headers if your provider exposes them\n",
    "for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "    if k in result[\"headers\"]:\n",
    "        print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1c568",
   "metadata": {},
   "source": [
    "## 2) A tiny test set (3 questions)\n",
    "\n",
    "We’ll cover:\n",
    "1. **Math reasoning** — inequality solving,\n",
    "2. **Common sense** — buoyancy/ice & water,\n",
    "3. **Logic** — a classic race-position puzzle.\n",
    "\n",
    "We also tightly constrain the required answer forms to enable simple auto‑grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0334e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9840398",
   "metadata": {},
   "source": [
    "## 3) Minimal evaluator\n",
    "\n",
    "We provide some example code to decide whether the agent outputs match the expected outputs, just to give you an idea of how evaluations can be done. You are free to use this code, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffddeb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2/3 correct\n",
      "❌ math_inequality: expected='8', got='4' (HTTP 200)\n",
      "✅ commonsense_ice: expected='stay the same', got='stay the same' (HTTP 200)\n",
      "✅ logic_race: expected='second', got='second' (HTTP 200)\n"
     ]
    }
   ],
   "source": [
    "# %% Simple normalization and evaluation helpers\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    # Remove surrounding punctuation and extra whitespace\n",
    "    s = re.sub(r\"[^\\w\\s\\-']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # Map common synonyms used in these tests\n",
    "    synonyms = {\n",
    "        \"unchanged\": \"stay the same\",\n",
    "        \"no change\": \"stay the same\",\n",
    "        \"same\": \"stay the same\",\n",
    "        \"second place\": \"second\",\n",
    "        \"2nd\": \"second\",\n",
    "        \"first place\": \"first\",\n",
    "        \"third place\": \"third\",\n",
    "    }\n",
    "    return synonyms.get(s, s)\n",
    "\n",
    "def extract_number(s: str):\n",
    "    # Returns first number occurrence as string if found, else None\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(r\"[-+]?\\d+(\\.\\d+)?\", s)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def grade(expected: str, got: str, kind: str) -> bool:\n",
    "    if kind == \"numeric\":\n",
    "        exp_num = extract_number(expected)\n",
    "        got_num = extract_number(got)\n",
    "        return (exp_num is not None) and (got_num == exp_num)\n",
    "    else:\n",
    "        return normalize_text(got) == normalize_text(expected)\n",
    "\n",
    "def evaluate_tests(tests, model=MODEL):\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            t[\"prompt\"],\n",
    "            system=\"You are a careful solver. Reply ONLY with the final answer, nothing else.\",\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        got = (r[\"text\"] or \"\").strip()\n",
    "        is_correct = grade(t[\"expected\"], got, t[\"type\"])\n",
    "        rows.append({\n",
    "            \"id\": t[\"id\"],\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": got,\n",
    "            \"correct\": is_correct,\n",
    "            \"status\": r[\"status\"],\n",
    "            \"error\": r[\"error\"],\n",
    "        })\n",
    "        # Tiny pacing to be polite to the API\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # Print a small report\n",
    "    correct = sum(1 for x in rows if x[\"correct\"])\n",
    "    print(f\"Score: {correct}/{len(rows)} correct\")\n",
    "    for x in rows:\n",
    "        mark = \"✅\" if x[\"correct\"] else \"❌\"\n",
    "        print(f\"{mark} {x['id']}: expected={x['expected']!r}, got={x['got']!r} (HTTP {x['status']})\")\n",
    "        if x[\"error\"]:\n",
    "            print(\"   error:\", x[\"error\"])\n",
    "    return rows\n",
    "\n",
    "results = evaluate_tests(tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6559aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # Fallback: simple normalization-based equality\n",
    "    norm = lambda s: re.sub(r\"\\s+\", \" \", (s or \"\").strip().lower())\n",
    "    return norm(prediction) == norm(expected_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bfad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ math_inequality: expected='8', got='4' (HTTP 200)\n",
      "✅ commonsense_ice: expected='stay the same', got='stay the same' (HTTP 200)\n",
      "✅ logic_race: expected='second', got='second' (HTTP 200)\n"
     ]
    }
   ],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    rows = []\n",
    "\n",
    "    for t in tests:\n",
    "        # 1) Get model prediction\n",
    "        r = call_model_chat_completions(\n",
    "            t[\"prompt\"],\n",
    "            system=\"You are a careful solver. Reply ONLY with the final answer, nothing else.\",\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "\n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"id\": t.get(\"id\", \"<unnamed>\"),\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": got,\n",
    "            \"correct\": bool(is_correct),\n",
    "            \"status\": r.get(\"status\"),\n",
    "            \"error\": r.get(\"error\"),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "        if verbose:\n",
    "            mark = \"✅\" if is_correct else \"❌\"\n",
    "            print(f\"{mark} {row['id']}: expected={row['expected']!r}, got={row['got']!r} (HTTP {row['status']})\")\n",
    "            if row[\"error\"]:\n",
    "                print(\"   error:\", row[\"error\"])\n",
    "\n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# Example:\n",
    "results_llm_judge = self_evaluate_tests(tests, verbose=True, model=MODEL, grader_model=MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124120c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 examples.\n",
      "=== RUNNING AGENT ON DEV DATA ===\n",
      "math\n",
      "\n",
      "Example 0: Domain=math\n",
      "INPUT: Let $ABCD$ be a convex quadrilateral with $AB = CD = 10$ , $BC = 14$ , and $AD = ...\n",
      "PREDICTED: quadrilateral\n",
      "EXPECTED:  112\n",
      "math\n",
      "\n",
      "Example 1: Domain=math\n",
      "INPUT: A tennis player computes her win ratio by dividing the number of matches she has ...\n",
      "PREDICTED: $\n",
      "EXPECTED:  164\n",
      "math\n",
      "\n",
      "Example 2: Domain=math\n",
      "INPUT: What is the product of the real roots of the equation $x^2 + 18x + 30 = 2 \\sqrt{ ...\n",
      "PREDICTED: 60\n",
      "EXPECTED:  20\n",
      "math\n",
      "\n",
      "Example 3: Domain=math\n",
      "INPUT: In $\\triangle ABC$ , $AB= 425$ , $BC=450$ , and $AC=510$ . An interior point $P$ ...\n",
      "PREDICTED: 42\n",
      "EXPECTED:  306\n",
      "math\n",
      "\n",
      "Example 4: Domain=math\n",
      "INPUT: How many even integers between 4000 and 7000 have four different digits? ...\n",
      "PREDICTED: all\n",
      "EXPECTED:  728\n",
      "math\n",
      "\n",
      "Example 5: Domain=math\n",
      "INPUT: For all positive integers $x$ , let \\[f(x)=\\begin{cases}1 &\\mbox{if }x = 1\\\\ \\fr ...\n",
      "PREDICTED: d(x)\n",
      "EXPECTED:  511\n",
      "math\n",
      "\n",
      "Example 6: Domain=math\n",
      "INPUT: Eight spheres of radius 100 are placed on a flat surface so that each sphere is  ...\n",
      "PREDICTED: Since\n",
      "EXPECTED:  152\n",
      "math\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m expected = example[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    111\u001b[39m domain = example[\u001b[33m\"\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m predicted = \u001b[43mget_classifaction_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m results.append({\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: i,\n\u001b[32m    117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m\"\u001b[39m: domain,\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexpected\u001b[39m\u001b[33m\"\u001b[39m: expected\n\u001b[32m    121\u001b[39m })\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Print a preview for each item\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mget_classifaction_prompt\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m classifaction.lower() == \u001b[33m'\u001b[39m\u001b[33mmath\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     48\u001b[39m     SYSTEM_PROMPT = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[33m    You are a Math assitant. Your job is to do a math problem. \u001b[39m\n\u001b[32m     50\u001b[39m \u001b[33m    The first thing needed is to undertsand what the problem is asking for.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m.strip()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     ans = \u001b[43mcall_model_chat_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     ans = ans.get(\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m).split()[-\u001b[32m1\u001b[39m]\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m classifaction.lower() == \u001b[33m'\u001b[39m\u001b[33mgeneral knowledge\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mcall_model_chat_completions\u001b[39m\u001b[34m(prompt, system, model, temperature, timeout)\u001b[39m\n\u001b[32m     30\u001b[39m payload = {\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m128\u001b[39m,\n\u001b[32m     38\u001b[39m }\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     status = resp.status_code\n\u001b[32m     43\u001b[39m     hdrs   = \u001b[38;5;28mdict\u001b[39m(resp.headers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 3 methods implamented here \n",
    "#1st method better prompt the bot to ensure that the prblem gets solved especailly the math ones\n",
    "\n",
    "def get_classifaction(question):\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    You are a helpful assitant. \n",
    "    This is very important please classify each problem into a category Math, General knowledge, predictions, and planning. Make sure the classification is correct and true.\n",
    "    It is very important that the classifcation is correct and is about what the problem is describing make sure that is the case.\n",
    "\n",
    "    This is very important as well please do not give the answers here this is jus classifcations for the problems. Also use the tags that I gave. \n",
    "    This is very important and must be followed. Please give one word as the repsonse and that is the answer. \n",
    "    \"\"\".strip()\n",
    "    prompt = 'Question' + question\n",
    "    \n",
    "    classifaction = call_model_chat_completions(prompt=prompt, system=SYSTEM_PROMPT, model=MODEL, temperature = 0.0, timeout = 60)\n",
    "    return classifaction.get('text')\n",
    "    \n",
    "    \n",
    "def get_classifaction_prompt(question):\n",
    "    classifaction = get_classifaction(question)\n",
    "    print(classifaction)\n",
    "    ans = ''\n",
    "    if classifaction.lower() == 'math':\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        You are a Math assitant. Your job is to do a math problem. \n",
    "        The first thing needed is to undertsand what the problem is asking for.\n",
    "        Once that is figured out solve the problem in steps until a soltion is reached. Do not display each step!\n",
    "        Redo the problem multiple times to make sure the correct answer is gotten. \n",
    "        Return the soltion like WHERE THE ANSWER IS ON A NEWLINE MAKE SURE THIS IS ON A NEW LINE BY ITSELF\n",
    "        AT THE END OF calcutions MAKE SURE ONLY the ANSWER IS OUTPUT\n",
    "        \n",
    "        \"\"\".strip()\n",
    "        ans = call_model_chat_completions(prompt=question, system=SYSTEM_PROMPT, model=MODEL, temperature = 0.0, timeout = 60)\n",
    "        ans = ans.get('text').split()[-1]\n",
    "    elif classifaction.lower() == 'general knowledge':\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        You are a assistent that knows lot of general knowledge. Your job is to answer the question correctly\n",
    "        The first thing needed is to undertsand what the problem is asking for.\n",
    "        Respond to the problem with only one answer and make sure the answer is correct \n",
    "        \n",
    "         \"\"\".strip()\n",
    "        ans = call_model_chat_completions(prompt=question, system=SYSTEM_PROMPT, model=MODEL, temperature = 0.0, timeout = 60)\n",
    "        ans = ans.get('text')\n",
    "    elif classifaction.lower() == 'predictions':\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        Your a predicting agent. \n",
    "        Use the correct data when making the predicitions and make sure they are correct \n",
    "        Make sure the anser is the answer only\n",
    "        \"\"\".strip()\n",
    "        ans = call_model_chat_completions(prompt=question, system=SYSTEM_PROMPT, model=MODEL, temperature = 0.0, timeout = 60)\n",
    "        ans = ans.get('text')\n",
    "    elif classifaction.lower() == 'planning':\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        You are a planning agent. The problems will be given with have to do with planning the order of events.\n",
    "        You will need to figure out what comes first and how they work with the information provided.\n",
    "        Make sure the output is only the answer and nothing but the answer\n",
    "        \"\"\".strip()\n",
    "        ans = call_model_chat_completions(prompt=question, system=SYSTEM_PROMPT, model=MODEL, temperature = 0.0, timeout = 60)\n",
    "        ans = ans.get('text')\n",
    "    else:\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        You are a assistent. Given the input of the problem make sure to thoughouly go through the problem.\n",
    "        Make sure to correctly answer the given problem.\n",
    "        The output should any be the answer only return the answer.\n",
    "        \"\"\".strip()\n",
    "        ans = call_model_chat_completions(prompt=question, system=SYSTEM_PROMPT, model=MODEL, temperature = 0.0, timeout = 60)\n",
    "        ans = ans.get('text')\n",
    "    return ans\n",
    "         \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"cse476_final_project_dev_data.json\", \"r\") as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "print(\"Loaded\", len(dev_data), \"examples.\")\n",
    "print(\"=== RUNNING AGENT ON DEV DATA ===\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, example in enumerate(dev_data):\n",
    "    question = example[\"input\"]\n",
    "    expected = example[\"output\"]\n",
    "    domain = example[\"domain\"]\n",
    "\n",
    "    predicted = get_classifaction_prompt(question)\n",
    "\n",
    "    results.append({\n",
    "        \"index\": i,\n",
    "        \"domain\": domain,\n",
    "        \"input\": question,\n",
    "        \"predicted\": predicted,\n",
    "        \"expected\": expected\n",
    "    })\n",
    "\n",
    "    # Print a preview for each item\n",
    "    print(f\"\\nExample {i}: Domain={domain}\")\n",
    "    print(\"INPUT:\", question[:80], \"...\")\n",
    "    print(\"PREDICTED:\", predicted)\n",
    "    print(\"EXPECTED: \", expected)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4489ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
